{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65a8f28",
   "metadata": {},
   "source": [
    "# Chapter1: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26419cb3",
   "metadata": {},
   "source": [
    "<b>Linear regression</b> is often considered the foundation of machine learning, and I vividly recall taking the Stats304 class at NU, which marked the beginning of my journey into machine learning and data science. Although the algorithm itself may seem straightforward, it is rooted in extensive statistical knowledge. Even today, linear regression finds widespread application in finance, marketing, and various other industries. It's worth noting that simpler models sometimes outperform complex and large models, demonstrating the effectiveness of simplicity in certain contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e6752",
   "metadata": {},
   "source": [
    "## Basic Assumptions of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513df962",
   "metadata": {},
   "source": [
    "- Linearity: Linear relationship of X\n",
    "- Normality: normally distributed X\n",
    "- Multicollinearity: independent X\n",
    "- Homoscedasticity: variance constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a459e511",
   "metadata": {},
   "source": [
    "## Using stats Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5298e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  GRADE   R-squared:                       0.416\n",
      "Model:                            OLS   Adj. R-squared:                  0.353\n",
      "Method:                 Least Squares   F-statistic:                     6.646\n",
      "Date:                Wed, 14 Jun 2023   Prob (F-statistic):            0.00157\n",
      "Time:                        22:46:25   Log-Likelihood:                -12.978\n",
      "No. Observations:                  32   AIC:                             33.96\n",
      "Df Residuals:                      28   BIC:                             39.82\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "GPA            0.4639      0.162      2.864      0.008       0.132       0.796\n",
      "TUCE           0.0105      0.019      0.539      0.594      -0.029       0.050\n",
      "PSI            0.3786      0.139      2.720      0.011       0.093       0.664\n",
      "const         -1.4980      0.524     -2.859      0.008      -2.571      -0.425\n",
      "==============================================================================\n",
      "Omnibus:                        0.176   Durbin-Watson:                   2.346\n",
      "Prob(Omnibus):                  0.916   Jarque-Bera (JB):                0.167\n",
      "Skew:                           0.141   Prob(JB):                        0.920\n",
      "Kurtosis:                       2.786   Cond. No.                         176.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "dataset = sm.datasets.spector.load()\n",
    "\n",
    "#adding a constant\n",
    "x = sm.add_constant(dataset.exog, prepend=False)\n",
    "y = dataset.endog\n",
    "\n",
    "#performing the regression\n",
    "model = sm.OLS(y, x).fit()\n",
    "\n",
    "# Result of statsmodels \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd06c4",
   "metadata": {},
   "source": [
    "## Using SkLearn Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d98b95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46385168, 0.01049512, 0.37855479, 0.        ])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "x = dataset.exog\n",
    "model = LinearRegression().fit(x, y)\n",
    "y_pred = model.predict(x)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590943d4",
   "metadata": {},
   "source": [
    "## Standardize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4419387",
   "metadata": {},
   "source": [
    "To ensure accurate analysis and modeling, it is crucial to normalize or standardize data when the feature distributions and scales are significantly different. By applying appropriate normalization or standardization techniques, we can achieve a more reliable and meaningful analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cd8eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b8e08f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  GRADE   R-squared:                       0.416\n",
      "Model:                            OLS   Adj. R-squared:                  0.353\n",
      "Method:                 Least Squares   F-statistic:                     6.646\n",
      "Date:                Wed, 14 Jun 2023   Prob (F-statistic):            0.00157\n",
      "Time:                        22:52:18   Log-Likelihood:                -12.978\n",
      "No. Observations:                  32   AIC:                             33.96\n",
      "Df Residuals:                      28   BIC:                             39.82\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.2131      0.074      2.864      0.008       0.061       0.365\n",
      "x2             0.0403      0.075      0.539      0.594      -0.113       0.194\n",
      "x3             0.1878      0.069      2.720      0.011       0.046       0.329\n",
      "const          0.3438      0.069      5.011      0.000       0.203       0.484\n",
      "==============================================================================\n",
      "Omnibus:                        0.176   Durbin-Watson:                   2.346\n",
      "Prob(Omnibus):                  0.916   Jarque-Bera (JB):                0.167\n",
      "Skew:                           0.141   Prob(JB):                        0.920\n",
      "Kurtosis:                       2.786   Cond. No.                         1.53\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "x_scaled = sm.add_constant(x_scaled, prepend=False)\n",
    "model = sm.OLS(y, x_scaled).fit()\n",
    "# Result of statsmodels \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b0a1a0",
   "metadata": {},
   "source": [
    "## From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ede53090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class LinearRegression() :\n",
    "    def __init__( self, learning_rate, iterations ) :\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "          \n",
    "    # Function for model training    \n",
    "    def fit(self,X,Y) :\n",
    "        self.m, self.n = X.shape\n",
    "        # weight initialization\n",
    "        self.W = np.zeros(self.n)\n",
    "        self.b = 0\n",
    "         \n",
    "        self.X = X   \n",
    "        self.Y = Y\n",
    "        # gradient descent learning\n",
    "        for i in range( self.iterations ) :\n",
    "            self.update_weights()\n",
    "        return self\n",
    "      \n",
    "    # Function for updating weights\n",
    "    def update_weights( self ) :           \n",
    "        Y_pred = self.predict( self.X )\n",
    "        # calculate gradients  \n",
    "        dW = - ( 2 * ( self.X.T ).dot( self.Y - Y_pred )  ) / self.m\n",
    "        db = - 2 * np.sum( self.Y - Y_pred ) / self.m \n",
    "        # update weights\n",
    "        self.W = self.W - self.learning_rate * dW\n",
    "        self.b = self.b - self.learning_rate * db\n",
    "        return self\n",
    "      \n",
    "    # Inferences\n",
    "    def predict( self, X ) :\n",
    "        return X.dot( self.W ) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8838140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained W: 0.21 0.04 0.19\n",
      "Trained b: 0.34\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(iterations = 3000, learning_rate = 0.001 )\n",
    "model.fit(x_scaled,y)\n",
    "# Prediction on test set\n",
    "y_pred = model.predict(x_scaled)\n",
    "print( \"Trained W:\", round( model.W[0], 2 ),round( model.W[1], 2 ),round( model.W[2], 2 ) )\n",
    "print( \"Trained b:\", round( model.b, 2 ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
