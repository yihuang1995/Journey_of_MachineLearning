{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "balanced-mainland",
   "metadata": {},
   "source": [
    "XGBoost has been recognized as one of the most powerful machine learning models for nearly a decade. It holds a special place in my preferences, alongside LightGBM, as a top non-deep-learning model.\n",
    "\n",
    "In many industries, XGBoost proves to be a highly effective classification/regression model, offering substantial power and accuracy.\n",
    "\n",
    "Majority of Kaggle's winning solutions in the tabular contexts use boosting trees as a fundamental component in their final models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "historic-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-operator",
   "metadata": {},
   "source": [
    "In this notebook, I will provide examples and demonstrate the major hyperparameters of the XGBoost model, as well as guide you on how to effectively tune them for optimal performance. \n",
    "\n",
    "Tuning hyperparameters can indeed be a time-consuming task. To streamline and automate this process, many practitioners, including myself, rely on tools like Optuna:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-military",
   "metadata": {},
   "source": [
    "### General Parameters\n",
    "\n",
    "**booster**: \n",
    "* default 'gbtree'. Can be 'gbtree', 'gblinear' or 'dart'\n",
    "* dart: Gradient Boosting Decision Trees with Dropout, dropout regularization. Further prevent overfitting.\n",
    "* gbliner: I didn't even notice before that Xgboost can support linear model as boosting method...\n",
    "\n",
    "**verbosity**: print level\n",
    "\n",
    "**nthread**: number parallel threads for faster computing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-confidentiality",
   "metadata": {},
   "source": [
    "### Booster Parameters\n",
    "\n",
    "**eta**: learning rate\n",
    "* Step size shrinkage used in update to prevents overfitting, default=0.3\n",
    "* After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "* range : [0,1]\n",
    "* Typical final values : 0.01-0.2.\n",
    "* [More explainations](https://medium.com/data-design/let-me-learn-the-learning-rate-eta-in-xgboost-d9ad6ec78363)\n",
    "\n",
    "**gamma**\n",
    "* Minimum loss reduction required to make a further partition on a leaf node of the tree. \n",
    "* The larger gamma is, the more conservative the algorithm will be.(Prevent overfitting or cause underfitting)\n",
    "* Range: [0,âˆž]\n",
    "\n",
    "**max_depth** \n",
    "* Maximum depth of a tree. Deeper tree could cause overfitting \n",
    "* default=6, typical value depends on the complexity of the task, normally 10 is pretty deep\n",
    "\n",
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-guidance",
   "metadata": {},
   "source": [
    "References:\n",
    "* https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "* https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
