{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "balanced-mainland",
   "metadata": {},
   "source": [
    "XGBoost has been recognized as one of the most powerful machine learning models for nearly a decade. It holds a special place in my preferences, alongside LightGBM, as a top non-deep-learning model.\n",
    "\n",
    "In many industries, XGBoost proves to be a highly effective classification/regression model, offering substantial power and accuracy.\n",
    "\n",
    "Majority of Kaggle's winning solutions in the tabular contexts use boosting trees as a fundamental component in their final models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-operator",
   "metadata": {},
   "source": [
    "In this notebook, I will provide examples and demonstrate the major hyperparameters of the XGBoost model, as well as guide you on how to effectively tune them for optimal performance. \n",
    "\n",
    "Tuning hyperparameters can indeed be a time-consuming task. To streamline and automate this process, many practitioners, including myself, rely on tools like Optuna:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-military",
   "metadata": {},
   "source": [
    "### General Parameters\n",
    "\n",
    "**booster**: \n",
    "* default 'gbtree'. Can be 'gbtree', 'gblinear' or 'dart'\n",
    "* dart: Gradient Boosting Decision Trees with Dropout, dropout regularization. Further prevent overfitting.\n",
    "* gbliner: I didn't even notice before that Xgboost can support linear model as boosting method...\n",
    "\n",
    "**verbosity**: print level\n",
    "\n",
    "**nthread**: number parallel threads for faster computing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-confidentiality",
   "metadata": {},
   "source": [
    "### Booster Parameters\n",
    "\n",
    "**eta**: learning rate\n",
    "* tree1 + eta * (tree2 + eta * (...))\n",
    "* Step size shrinkage used in update to prevents overfitting, default=0.3\n",
    "* After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "* range : [0,1]\n",
    "* Typical final values : 0.01-0.2.\n",
    "* [More explainations](https://medium.com/data-design/let-me-learn-the-learning-rate-eta-in-xgboost-d9ad6ec78363)\n",
    "\n",
    "**gamma**\n",
    "* Minimum loss reduction (information gain) required to make a further partition on a leaf node of the tree. \n",
    "* The larger gamma is, the more conservative the algorithm will be.(Prevent overfitting or cause underfitting)\n",
    "* Range: [0,âˆž]\n",
    "\n",
    "**max_depth** \n",
    "* Maximum depth of a tree. Deeper tree could cause overfitting \n",
    "* default=6, typical value depends on the complexity of the task, normally 10 is pretty deep\n",
    "\n",
    "**min_child_weight**\n",
    "* minimum sum of weights in a child node (aka. cover)\n",
    "* Prevent overfitting\n",
    "\n",
    "**subsample**\n",
    "* fraction of observations to be randomly samples for each tree\n",
    "* Range 0-1 (typical values: 0.5-1)\n",
    "\n",
    "**colsample_bytree, colsample_bylevel, colsample_bynode [default=1]**\n",
    "* fraction of observations to be randomly samples for each tree/node/level\n",
    "\n",
    "**Lambda**\n",
    "* L2 regularization term in the simularity score calcuilation's denominator\n",
    "* Prevent overfitting\n",
    "\n",
    "**Alpha**\n",
    "* L1 regularization term in the simularity score calcuilation\n",
    "* Prevent overfitting\n",
    "\n",
    "**tree_method**\n",
    "* auto (default), exact, approx, hist, gpu_hist\n",
    "* How to split the tree\n",
    "\n",
    "**scale_pos_weight**\n",
    "* For imbalanced data\n",
    "* A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence.\n",
    "* A typical value to consider: sum(negative instances) / sum(positive instances)\n",
    "\n",
    "**max_leaves**\n",
    "* Maximum number of nodes to be added (default 0). \n",
    "* Only relevant when grow_policy=lossguide is set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-guidance",
   "metadata": {},
   "source": [
    "References:\n",
    "* https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "* https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initate the model\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:pseudohubererror',\n",
    "    #objective='reg:squarederror',\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=4999,\n",
    "    learning_rate=0.0075,\n",
    "    max_leaves = 17,\n",
    "    subsample=0.50,\n",
    "    colsample_bytree=0.50,\n",
    "    max_bin=4096,\n",
    "    n_jobs=2,\n",
    "    eval_metric='mae',\n",
    "    early_stopping_rounds=70,\n",
    ")\n",
    "\n",
    "train_indices = ...\n",
    "valid_indices = ...\n",
    "\n",
    "#fit with the training data\n",
    "model.fit(\n",
    "    df.loc[train_indices, features],\n",
    "    df.loc[train_indices, 'target'],\n",
    "    eval_set=[(df.loc[valid_indices, features], df.loc[valid_indices, 'target'])],\n",
    "    verbose=500,\n",
    ")\n",
    "#predict validation data\n",
    "ypred = model.predict(df.loc[valid_indices, features])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
