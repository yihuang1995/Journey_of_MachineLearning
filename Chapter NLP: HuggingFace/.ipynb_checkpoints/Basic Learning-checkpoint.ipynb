{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "civilian-stick",
   "metadata": {},
   "source": [
    "HuggingFace's transformers library has greatly facilitated our work in creating LLM model structures, particularly with my main usage of PyTorch. It has streamlined the process and automated many tasks, saving us from reinventing the wheel and allowing us to focus on the core aspects of our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "immediate-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sexual-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "turned-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  1049,  2478, 17662, 12172,  2000, 25416, 21898,\n",
      "          2026, 17953,  2361,  3716,   102],\n",
      "        [  101,  1045,  2293,  3698,  4083,   999,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I'm using HuggingFace to refresh my NLP knowledge\",\n",
    "    \"I love machine learning!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coastal-living",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intellectual-property",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "# output is hiddenstate which can be used as downstream job inputs\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opponent-classic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.2838,  0.1341,  0.7624,  ..., -0.0148,  0.5160, -0.3466],\n",
       "         [ 0.8246,  0.5177,  0.7209,  ..., -0.0891,  0.5928, -0.0592],\n",
       "         [ 0.8274,  0.4883,  0.4438,  ...,  0.2366, -0.2426, -0.6068],\n",
       "         ...,\n",
       "         [-0.1527,  0.2495,  0.4099,  ..., -0.3064, -0.0788, -0.0025],\n",
       "         [ 0.2095,  0.4228,  0.9903,  ...,  0.1401,  0.4759, -0.2111],\n",
       "         [ 0.5834,  0.6062,  0.4520,  ...,  0.4326, -0.0893, -0.4952]],\n",
       "\n",
       "        [[ 0.6307, -0.0675, -0.1219,  ...,  0.3686,  0.8636, -0.4454],\n",
       "         [ 1.0353,  0.1182, -0.3107,  ...,  0.3189,  0.9273, -0.1798],\n",
       "         [ 1.1944,  0.4115,  0.1433,  ...,  0.2893,  1.0146, -0.2526],\n",
       "         ...,\n",
       "         [ 0.4094,  0.1410,  0.1120,  ...,  0.5282,  0.6460, -0.3872],\n",
       "         [ 0.6244,  0.3469, -0.1221,  ...,  0.1465,  0.7602, -0.1418],\n",
       "         [ 0.6210,  0.1682,  0.0198,  ...,  0.4434,  0.5817, -0.4767]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-learning",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "flush-albania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# For classification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "first-modern",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7711,  2.8365],\n",
       "        [-4.2119,  4.5484]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sealed-cooler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.6564e-03, 9.9634e-01],\n",
      "        [1.5681e-04, 9.9984e-01]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "average-young",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(predictions, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-joining",
   "metadata": {},
   "source": [
    "### Sequence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "oriented-simpson",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertForQuestionAnswering: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "graduate-hybrid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-0.3043, -0.2337, -0.0926, -0.1852, -0.0555, -0.0929, -0.1327, -0.1678,\n",
       "          0.0032, -0.0625, -0.0135,  0.0215,  0.0277,  0.0527, -0.0299],\n",
       "        [-0.1896, -0.0478,  0.1463, -0.0870, -0.0384, -0.0273, -0.0391,  0.0259,\n",
       "          0.0408,  0.0670, -0.0503,  0.0164,  0.0483,  0.0132, -0.0048]],\n",
       "       grad_fn=<CopyBackwards>), end_logits=tensor([[ 0.0609,  0.0902, -0.0635, -0.1254, -0.1044, -0.0873, -0.1234, -0.1236,\n",
       "          0.1952,  0.2212,  0.3397,  0.1109,  0.0489,  0.0885, -0.0533],\n",
       "        [-0.1319,  0.0147, -0.1183, -0.0720, -0.0228, -0.0718, -0.0561, -0.1151,\n",
       "         -0.1420, -0.1248, -0.1660, -0.2351, -0.2014, -0.0825, -0.1387]],\n",
       "       grad_fn=<CopyBackwards>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-globe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
